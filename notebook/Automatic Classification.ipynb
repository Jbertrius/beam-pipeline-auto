{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from math import *\n",
    "import logging\n",
    "\n",
    "\n",
    "# And some Machine Learning modules from scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import copy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from category_encoders import *\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Some modules for plotting and visualizing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# In[232]:\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.datasets import make_classification\n",
    "import category_encoders\n",
    "import functools\n",
    "import imblearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['flex', 'pays', 'fab', 'vds', 'chass', 'checkD', 'year', 'plant']\n",
    "clus_var = pd.read_csv('chassis/cluster_de_variante.csv') #Données sur les variantes\n",
    "df_mixed = pd.read_csv('chassis/df_mixed.csv') #Données mixtes composées de VIN français, suédois et espagnols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage des variantes n'ayant pas plus de 20 VIN\n",
    "min_instances = 20\n",
    "b = pd.DataFrame.from_dict(Counter(df_mixed['idvariante']), orient='index').sort_values(by=0)\n",
    "df_mixed = df_mixed[ df_mixed['idvariante'].isin( list(b[ b[0] > min_instances ].index ) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to loop classification \n",
    "\n",
    "# Cluster = désigne le cluster dont on calcule le modèle\n",
    "# df = Dataset utilisé pour la classification\n",
    "# y_col = Colonne de la cible à prédir (Peut etre les variantes ou les clusters de variantes)\n",
    "# resample = Booléen désignant si on doit équilibrer la distribution des classes ou non (Il s'agit de generer des elements factices pour eviter un biais dans le calcul)\n",
    "# cv = Booléen désignant si on doit rechercher les meilleurs hyperparametres ou non\n",
    "# col = Colonne des features à utiliser dans le calcul du modele\n",
    "# name = Nom donné au modele généré\n",
    "# filtering = Booléen désignant si on doit filtrer ou non le dataset d'entrée\n",
    "\n",
    "# L'utilisation du cv peut etre très couteux en temps, c'est pourquoi par défaut il est False.\n",
    "#\n",
    "# L'utilisation de resample peut egalement etre couteux en temps de calcul néanmoins il est True par défaut car affecte \n",
    "# beaucoup le taux de prédiction finale\n",
    "\n",
    "def CalculClassification(cluster='clus', df=df_mixed, y_col='class', resample = True, cv= False, col=col, name = None, filtering=True  ):\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    # Définition du logFile\n",
    "    logging.basicConfig(filename='calcul_chassis-{}.log'.format(str(dt.datetime.now().date())) ,level=logging.DEBUG)\n",
    "    logging.basicConfig(format='%(asctime)s : %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "    # Les 3 paramètres ci-dessus étaient en paramètres de la fonction mais on été fixé pour éviter un calcul trop long\n",
    "    # Néanmoins leur but est de rechercher pour chaque modele calculé, l'algorithme adéquat (entre Random Forest, Gradient Boosting, Bagging)\n",
    "    #\n",
    "    # one = désigne la variable permettant d'initialiser ou non la recherche du meilleur algo, si il est False, on lance la recherche\n",
    "    # sinon il ne considere qu'un algo au choix\n",
    "    #\n",
    "    #loop = permet de faire une boucle pour relancer un hyperparamétrage sur le meilleur algo trouvé\n",
    "    #clf = variable qui contiendra plus tard le meilleur modele \n",
    "    loop = 0\n",
    "    one = True\n",
    "    clf = None\n",
    "\n",
    "\n",
    "    #Filtering\n",
    "    if cluster == 'clus':\n",
    "        DF = df.drop(columns ='idvariante')\n",
    "        y_col = 'class'\n",
    "        \n",
    "    elif type(cluster) == int or type(cluster) == np.int64 :\n",
    "        if y_col == 'class':\n",
    "            if filtering:\n",
    "                DF = df[ df['class'] == cluster ].drop(columns = 'idvariante')\n",
    "            else:\n",
    "                DF = df.drop(columns = 'idvariante')\n",
    "        elif y_col == 'idvariante':\n",
    "            if filtering :\n",
    "                DF = df[ df['class'] == cluster ].drop(columns='class')\n",
    "            else :\n",
    "                DF = df.drop(columns = 'class')\n",
    "    else:\n",
    "        raise ValueError('A very specific bad thing happened')\n",
    "        \n",
    "    if not name:\n",
    "        name = cluster\n",
    "\n",
    "    #Encodage\n",
    "    logging.info('{}/{}'.format(name,'encodage') )\n",
    "    x_cols = DF.columns.drop(y_col)\n",
    "    \n",
    "    \n",
    "    y = DF[y_col]\n",
    "    X = DF[x_cols]\n",
    "\n",
    "    \n",
    "    display(DF.shape)\n",
    "    display(Counter(y))\n",
    "    \n",
    "    \n",
    "    ce_x = BinaryEncoder(cols=col).fit(X,y)\n",
    "    X_num = ce_x.transform(X)\n",
    "    joblib.dump(ce_x, 'chassi__s/encoder/enc_clus%s' % name )\n",
    "\n",
    "\n",
    "    #Label Encoding    \n",
    "    logging.info('{}/{}'.format(name,'label_encoding') )\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y_res = le.transform(y)\n",
    "    joblib.dump(le, 'chassi__s/label/label_clus%s' % name)\n",
    "\n",
    "    \n",
    "    #Resampling\n",
    "    logging.info('{}/{}'.format(name,'resampling') )\n",
    "    if resample:\n",
    "        #rus = imblearn.under_sampling.RandomUnderSampler(random_state=42)\n",
    "        rus = imblearn.over_sampling.SMOTE(n_jobs=-1, kind='borderline1')\n",
    "        X_res, Y_res = rus.fit_sample(X_num, y_res)\n",
    "    else:\n",
    "        X_res, Y_res = X_num, y_res\n",
    "\n",
    "\n",
    "    # Affichage distribution par classes\n",
    "    %matplotlib inline\n",
    "    nb_counts = Counter(Y_res)\n",
    "    tdf = pd.DataFrame.from_dict(nb_counts, orient='index').sort_values(by=0)\n",
    "\n",
    "    index = tdf.index.tolist()\n",
    "\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(range(len(index)), tdf[0].tolist())\n",
    "    ax.axhline(500) \n",
    "\n",
    "    plt.xlabel('Classe', fontsize=15)\n",
    "    plt.ylabel('Nb instances', fontsize=15)\n",
    "    plt.xticks(range(len(index)), index, fontsize=10, rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "    # Split train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X_res, Y_res, test_size=0.2, random_state=20,  stratify=Y_res)\n",
    "\n",
    "    # Training and evaluation\n",
    "    logging.info('{}/{}'.format(name,'training') )\n",
    "    \n",
    "    #while loop < 2: #Décommenter cette ligne implique la correction de l'indentation\n",
    "    \n",
    "    if one == True:\n",
    "        #clf = ensemble.GradientBoostingClassifier(n_estimators=100)\n",
    "        clf = ensemble.RandomForestClassifier(n_estimators=100, \n",
    "                                            criterion='entropy',\n",
    "                                            min_samples_split=3, \n",
    "                                            random_state = 42, \n",
    "                                            max_depth=25, \n",
    "                                            n_jobs=-1,  \n",
    "                                            class_weight=None)\n",
    "        \n",
    "        if cluster == 'clus':\n",
    "            clf = ensemble.BaggingClassifier(n_estimators=100)\n",
    "\n",
    "        if cv:\n",
    "            if 'max_depth' in clf.get_params():\n",
    "                grid = [{\n",
    "                    #'n_estimators': [ 100, 150],\n",
    "                    'max_depth': [ None,  15, 20 ],\n",
    "                    #'criterion': ['entropy', 'gini']\n",
    "                }]\n",
    "            elif 'max_samples' in clf.get_params():\n",
    "                grid = [{\n",
    "                    'n_estimators': [100, 200], \n",
    "                    'max_features': [ 1.0]\n",
    "                }]\n",
    "            cv = GridSearchCV(clf, grid, scoring='accuracy', cv=None)\n",
    "            cv.fit(X_train, y_train)\n",
    "            logging.info('{}/{}'.format(cluster,metrics.accuracy_score(y_test, cv.predict(X_test))) )\n",
    "            clf = cv\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            logging.info('{}/{}'.format(cluster, metrics.accuracy_score(y_test, clf.predict(X_test))) )\n",
    "\n",
    "        # Display Evaluation result\n",
    "        print(metrics.classification_report( y_test, clf.predict(X_test) ))\n",
    "\n",
    "        #Saving model\n",
    "        from sklearn.externals import joblib\n",
    "        joblib.dump(clf, 'chassi__s/model/cls_%s' % name)\n",
    "\n",
    "    else:\n",
    "        # Recherche du meilleur algo \n",
    "        pipeline = Pipeline([('est', ensemble.RandomForestClassifier())])\n",
    "\n",
    "        grid = [\n",
    "            {'est': [ensemble.BaggingClassifier()] },\n",
    "            {'est': [ensemble.RandomForestClassifier()] },\n",
    "            {'est': [ensemble.GradientBoostingClassifier()] },\n",
    "            {'est': [ensemble.AdaBoostClassifier()] },\n",
    "        ]\n",
    "\n",
    "        grid_test = [{\n",
    "            'est': [\\\n",
    "                    ensemble.BaggingClassifier(), \\\n",
    "                    ensemble.RandomForestClassifier(), \\\n",
    "                    ensemble.GradientBoostingClassifier(), \\\n",
    "                    ensemble.AdaBoostClassifier()\\\n",
    "                   ]\n",
    "        }]\n",
    "\n",
    "        gs = GridSearchCV(pipeline, grid, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "        errvals = np.array([])\n",
    "\n",
    "        this_err = metrics.accuracy_score(y_test, gs.predict(X_test))\n",
    "\n",
    "        print(this_err)\n",
    "        logging.info('{}/{}'.format(cluster,this_err) )\n",
    "        \n",
    "        clf = gs.best_estimator_.steps[0][1]\n",
    "        one = True\n",
    "\n",
    "    loop+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on determine les clusters qui ont plus de 100 variantes, pour ces cas un autre algorithme leur sera appliqué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highvar = [] # list des clusters ayant trop de variantes\n",
    "\n",
    "_count = {}\n",
    "for name, group in df_mixed.groupby([df_mixed['class'], df_mixed['idvariante']]):\n",
    "    try:\n",
    "        _count[ name[0] ] = _count[ name[0] ] + 1\n",
    "    except KeyError:\n",
    "        _count[ name[0] ] = 0\n",
    "for i,k in _count.items():\n",
    "    if k > 100:\n",
    "        highvar.append(i)\n",
    "\n",
    "# 'clus' désigne le valeur pour calculer le modele permettant de prédire les clusters de variantes\n",
    "# __list_clus contient les clusters ayant un nombre de variantes inférieur à 100\n",
    "__list_clus = ['clus'] + list(set(df_mixed['class'].unique()) - set(highvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in __list_clus:\n",
    "    if i == 'clus':\n",
    "        CalculClassification(cluster=i, resample=False)\n",
    "    else:\n",
    "        CalculClassification(cluster=i, y_col='idvariante')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculClassification(cluster=34, y_col='idvariante')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La fonction ci-dessous automatise le calcul des modeles pour les clusters plus de 100 variantes.\n",
    "En effet, au delà de 100 variantes, le modèle calculé devient très volumineux et moins précis.**\n",
    "\n",
    "**Pour ce faire on détermine d'autres clusters de variantes intermédiaires pour réduire le nombre de variantes le clusters.**\n",
    "\n",
    "*Ex: Cluster A = 120 Variantes => Cluster A-1 = 70 variantes et Cluster A-2 = 50 variantes*\n",
    "\n",
    "**On calcule ainsi un modele pour prédire ces nouveaux clusters et un autre modele pour prédire les variantes à l'intérieur de ces clusters.**\n",
    "\n",
    "**Pour determiner les clusters, on se base sur des attributs issues de la table des variantes, ces attributs sont choisis\n",
    "par rapport à leur aptitude à scinder de facon plus ou moins uniforme les variantes à l'interieur du cluster et aussi de garder une certaine cohérence métier.**\n",
    "\n",
    "*Ex: modele => regroupe les variantes dans des clusters ayant des modeles distincts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribut pour determiner les clusters intermédiaires\n",
    "__att = {}\n",
    "__att[7] = ['modele','vitessesnbr']\n",
    "__att[9] = ['modele','energie']\n",
    "__att[17] = ['carosserie']\n",
    "__att[11] = ['energie']\n",
    "__att[29] = ['modele', 'carosserie']\n",
    "\n",
    "#Fonction récursive qui calcule tous les modeles des clusters à variantes élevé ( cluster intermédiaire -> variante )\n",
    "def ClassiMultiNiveau(DF, attr_list, cluster, clus_var_loc, name=None ):\n",
    "    col = attr_list.pop(0)\n",
    "    \n",
    "    attr =  clus_var_loc[col].unique().tolist()\n",
    "\n",
    "    \n",
    "    for j,x in enumerate(attr):\n",
    "        variante_list = clus_var_loc[ clus_var_loc[col] == x ]['id'].tolist()\n",
    "        DF.loc[ DF['idvariante'].isin(variante_list), 'class' ] = j\n",
    "    \n",
    "    if not name:\n",
    "        rad = str(cluster)\n",
    "    else:\n",
    "        rad = name\n",
    "              \n",
    "    CalculClassification(cluster=i, df=DF, y_col='class', filtering=False, name=rad)\n",
    "    \n",
    "    for j in Counter(DF['class']):\n",
    "        display('%s------%s' % (rad,j))\n",
    "        DF_x = DF[ DF['class'] == j ]\n",
    "\n",
    "        if len( DF_x['idvariante'].unique()) > 90:\n",
    "            name = rad + '_%s'%j\n",
    "            \n",
    "            clus_var_loc = clus_var_loc[ clus_var_loc[col] == attr[j] ]\n",
    "            ClassiMultiNiveau( DF=DF_x, attr_list=attr_list, cluster= i, clus_var_loc=clus_var_loc, name=name)\n",
    "        else:\n",
    "            name = rad + '_%s'%j\n",
    "            CalculClassification(cluster=i, df=DF_x, y_col='idvariante', filtering=False, name=name)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i,k in __att.items():\n",
    "    \n",
    "    DF = df_mixed[ df_mixed['class'] == int(i) ]\n",
    "    ClassiMultiNiveau(DF=DF, attr_list=k, cluster=i, clus_var_loc = clus_var[ clus_var['cluster'] == i ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
